{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#category_id_list = [398,401,402,403,404,413,419,522,421,432,433,434,435,436,437,438,439,440,441,442,443,444,449,523,\n",
    "#               343,348,357,364,369,374,375,517,528,390,391,392,393,394,395,396,518,451,456,457,464,465,470,473,\n",
    "#               474,477,514,519,527,480,481,482,483,484,516,521,486,487,488,489,490,491,492,493,494,524,496,497,\n",
    "#               498,499,500,501,502,505,525]\n",
    "#region_list = ['beijing','shanghai','shenzhen','guangzhou','hangzhou','chengdu','xian','wuhan','ningbo']\n",
    "category_id_list = [398,401,402]\n",
    "region_list = ['ningbo']\n",
    "fieldnames = ['tutor_id','topic_id','tutor_name','tutor_title','response_day','num_meet','num_wanna_see',\n",
    "              'acceptance_rate','num_product','tutor_intro','len_tutor_intro','earliest_review_day','day_opened']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_title_link(region, cat_id):\n",
    "    page = 1\n",
    "    while True:\n",
    "        res = requests.get(\"http://www.zaih.com/topics/?category_id=%s&city=%s&page=%d\" % (cat_id, region, page))\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        for link in soup.select(\"a.topic-tutor-link\"):\n",
    "            yield link['href']\n",
    "        if soup.select(\"span.icon.icon-next\") == []:\n",
    "            break\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_tutor_page(region, cat_id, tutor_link):\n",
    "    tutor_id = str(tutor_link[8:16])\n",
    "    topic_id = str(tutor_link[23:31])\n",
    "    res = requests.get(\"http://www.zaih.com%s\" % tutor_link)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    tutor_name = soup.select(\"h1.tutor-name\")[0].text    \n",
    "    tutor_title = soup.select(\"p.tutor-title\")[0].text\n",
    "    temp = soup.select(\"span.highlight\")\n",
    "    response_day = temp[0].text\n",
    "    num_meet = temp[1].text\n",
    "    num_wanna_see = temp[2].text\n",
    "    if len(temp) == 4:\n",
    "        acceptance_rate = temp[3].text\n",
    "    else:\n",
    "        acceptance_rate = \"\"\n",
    "    num_product = len(soup.select(\"li.topic-item\"))\n",
    "    tutor_intro = \"\"\n",
    "    for intro in soup.select(\"div.about-tutor-summary\"):\n",
    "        tutor_intro += intro.text \n",
    "    tutor_intro = tutor_intro.strip()\n",
    "    len_tutor_intro = len(tutor_intro)\n",
    "    \n",
    "    # convert unicode to string so it can be written to csv\n",
    "    tutor_name = unicodedata.normalize('NFKD', tutor_name).encode('gb2312','ignore')\n",
    "    tutor_intro = unicodedata.normalize('NFKD', tutor_intro).encode('ascii','ignore')\n",
    "    tutor_title = unicodedata.normalize('NFKD', tutor_title).encode('ascii','ignore')\n",
    "    response_day = unicodedata.normalize('NFKD', response_day).encode('ascii','ignore')\n",
    "    num_meet = unicodedata.normalize('NFKD', num_meet).encode('ascii','ignore')\n",
    "    num_wanna_see = unicodedata.normalize('NFKD', num_wanna_see).encode('ascii','ignore')\n",
    "    if acceptance_rate != '':\n",
    "        acceptance_rate = unicodedata.normalize('NFKD', acceptance_rate).encode('ascii','ignore')\n",
    "    \n",
    "    # get date of earliest review and the number of day_opened\n",
    "    res2 = requests.get(\"http://www.zaih.com/apis/open/topic_reviews?tutor_id=85071906&offset=0&limit=10000&sort_by=latest\")\n",
    "    soup2 = BeautifulSoup(res2.text)\n",
    "    temp_str_list = unicodedata.normalize('NFKD', soup2.select(\"body\")[0].text).encode('ascii','ignore').split(\"date_created\")\n",
    "    if len(temp_str_list) == 1:\n",
    "        earliest_review_day = \"No review yet\"\n",
    "        day_opened = -1\n",
    "    else:\n",
    "        earliest_review_day = temp_str_list[-1][4:14]\n",
    "        datetime_object = datetime.strptime(earliest_review_day,'%Y-%m-%d')\n",
    "        time_now = datetime.now()\n",
    "        day_opened = (time_now - datetime_object).days\n",
    "        \n",
    "    # crawl tutor image\n",
    "    image_link = soup.select(\"img\")[0]['src']\n",
    "    urllib.urlretrieve(\"http:%s\" % image_link, \"tutor_image/%s_image.jpg\" % tutor_id)\n",
    "    \n",
    "    \n",
    "    print type(tutor_id)\n",
    "    print type(topic_id)\n",
    "    print type(tutor_name)\n",
    "    print type(tutor_title)\n",
    "    print type(response_day)\n",
    "    print type(num_meet)\n",
    "    print type(num_wanna_see)\n",
    "    print type(acceptance_rate)\n",
    "    print type(num_product)\n",
    "    print type(tutor_intro)\n",
    "    print type(len_tutor_intro)\n",
    "    print type(earliest_review_day)\n",
    "    print type(day_opened)\n",
    "    # write into a csv file  \n",
    "    cat_shop_list = {'tutor_id': tutor_id, 'topic_id': topic_id, 'tutor_name': tutor_name, 'region': region, \n",
    "                     'tutor_title': tutor_title, 'cat_id': cat_id,'response_day': response_day,'num_meet': num_meet, \n",
    "                     'num_wanna_see': num_wanna_see,'acceptance_rate': acceptance_rate, 'num_product': num_product, \n",
    "                     'tutor_intro': tutor_intro,'len_tutor_intro': len_tutor_intro, 'earliest_review_day': earliest_review_day, 'day_opened':day_opened}\n",
    "    with open('file.csv', 'r+b') as f:\n",
    "        header = next(csv.reader(f))\n",
    "        dict_writer = csv.DictWriter(f, header, -1)\n",
    "        dict_writer.writerow(cat_shop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not done\n",
    "def crawl_product(tutor_link):\n",
    "    tutor_id = tutor_link[8:16]\n",
    "    res = requests.get(\"http://www.zaih.com%s\" % tutor_link)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    for product in soup.select(\"li.topic-item\"):\n",
    "        # crawl sth\n",
    "        # export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not done\n",
    "def crawl_review(tutor_link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n",
      "<type 'str'>\n",
      "<type 'int'>\n"
     ]
    }
   ],
   "source": [
    "# main program starts\n",
    "\n",
    "# for each pair of region and catalog_id\n",
    "global_topic_title_link = []\n",
    "for region in region_list:\n",
    "    for cat_id in category_id_list:\n",
    "        # reset the list\n",
    "        topic_title_link = []\n",
    "        \n",
    "        # get all mentor page link under the same region and categories\n",
    "        topic_title_link += get_topic_title_link(region, cat_id)\n",
    "        global_topic_title_link += topic_title_link\n",
    "        \n",
    "        # crawl each mentor's page\n",
    "        for tutor_link in topic_title_link:\n",
    "            crawl_tutor_page(region, cat_id, tutor_link)\n",
    "            \n",
    "    # get unique tutor link, avoid crawling repeated product information\n",
    "    # if memory overflow, write this to a file and then clear this\n",
    "    global_topic_title_link = set(global_topic_title_link)\n",
    "\n",
    "#for tutor_link in global_topic_title_link:\n",
    " #   crawl_product(tutor_link)\n",
    "  #  crawl_review(tutor_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
